{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import model_selection\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previously processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reload data that was processed last time\n",
    "pickle_file = 'data/training_data.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    \n",
    "    X_small = save['X_small']\n",
    "    y_small = save['y_small']\n",
    "    \n",
    "    del save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we need a new initiative from the commission on this\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(word for word in X_small[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il nous faut une nouvelle initiative de la commission Ã  ce sujet\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(word for word in y_small[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For ease of training, only keep sentences shorter than 30 words\n",
    "X_len = [len(sentence) for sentence in X_small]\n",
    "y_len = [len(sentence) for sentence in y_small]\n",
    "\n",
    "min_len = 2\n",
    "max_len = 30\n",
    "\n",
    "X_to_keep_ix = np.where((np.array(X_len) >= min_len) & (np.array(X_len) <= max_len))\n",
    "y_to_keep_ix = np.where((np.array(y_len) >= min_len) & (np.array(y_len) <= max_len))\n",
    "\n",
    "to_keep_ix = list(set(np.intersect1d(X_to_keep_ix, y_to_keep_ix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_small = X_small[to_keep_ix]\n",
    "y_small = y_small[to_keep_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 30, 2, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "X_len = [len(sentence) for sentence in X_small]\n",
    "y_len = [len(sentence) for sentence in y_small]\n",
    "\n",
    "min(X_len), max(X_len), min(y_len), max(y_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word-to-index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word_to_id_mapping(data, max_vocab_size = 20000):\n",
    "    counter = collections.Counter(np.hstack(data))\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    # Pick the most common ones\n",
    "    count_pairs = count_pairs[:max_vocab_size]\n",
    "\n",
    "    # Add 'ZERO', 'GO', and 'UNK'\n",
    "    # It is important to add 'ZERO' in the beginning\n",
    "    # to make sure zero padding does not interfere with existing words\n",
    "    count_pairs.insert(0, ('GO', 0))\n",
    "    count_pairs.insert(0, ('ZERO', 0))\n",
    "    count_pairs.append(('UNK', 0))\n",
    "\n",
    "    # Create mapping for both directions\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    id_to_word = dict(zip(range(len(words)), words))\n",
    "    \n",
    "    # Map words to indexes\n",
    "    data_id = [[word_to_id[word] if word in word_to_id else word_to_id['UNK'] for word in sentence] for sentence in data]\n",
    "    \n",
    "    return word_to_id, id_to_word, data_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_word_to_id, X_id_to_word, X_id = create_word_to_id_mapping(X_small)\n",
    "y_word_to_id, y_id_to_word, y_id = create_word_to_id_mapping(y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70211 70211 20003 20003\n"
     ]
    }
   ],
   "source": [
    "print(len(X_id), len(y_id), len(X_word_to_id), len(y_word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_vocab_size, y_vocab_size = len(X_word_to_id), len(y_word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we need a new initiative from the commission on this\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([X_id_to_word[i] for i in X_id[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il nous faut une nouvelle initiative de la commission Ã  ce sujet\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([y_id_to_word[i] for i in y_id[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad zeros to make sentences equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_id_padded = pad_sequences(X_id, maxlen=max_len, padding='post')\n",
    "y_id_padded = pad_sequences(y_id, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverage pre-trained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English word vectors downloaded from https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code stolen from https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "embeddings_index_en = {}\n",
    "f = open('data/glove.6B/glove.6B.200d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index_en[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French word vectors downloaded from http://fauconnier.github.io/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index_fr = word2vec.KeyedVectors.load_word2vec_format(\n",
    "    'data/frWac_non_lem_no_postag_no_phrase_200_skip_cut100.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map words to pre-trained embeddings\n",
    "def map_word_to_pretrained_embedding(embeddings_index, embedding_size, word_to_id):\n",
    "    vocab_size = len(word_to_id)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "    \n",
    "    # Keep a running count of matched words\n",
    "    found = 0\n",
    "    \n",
    "    for word, i in word_to_id.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            found += 1\n",
    "        else:\n",
    "            # Words not found in embedding index will be randomly initialized\n",
    "            embedding_matrix[i] = np.random.normal(size=(embedding_size, ))\n",
    "\n",
    "    return embedding_matrix, found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20003, 200), 0.8669199620056991)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embeddings, X_found = map_word_to_pretrained_embedding(embeddings_index_en, 200, X_word_to_id)\n",
    "X_embeddings.shape, X_found / X_embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20003, 200), 0.8486726990951358)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_embeddings, y_found = map_word_to_pretrained_embedding(embeddings_index_fr, 200, y_word_to_id)\n",
    "y_embeddings.shape, y_found / y_embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'data/training_data.pickle'\n",
    "\n",
    "f = open(pickle_file, 'wb')\n",
    "save = {\n",
    "    'X_small': X_small,\n",
    "    'y_small': y_small,\n",
    "    'X_word_to_id': X_word_to_id,\n",
    "    'X_id_to_word': X_id_to_word,\n",
    "    'y_word_to_id': y_word_to_id,\n",
    "    'y_id_to_word': y_id_to_word,\n",
    "    'X_id_padded': X_id_padded,\n",
    "    'y_id_padded': y_id_padded,\n",
    "    'X_embeddings': X_embeddings,\n",
    "    'y_embeddings': y_embeddings\n",
    "}\n",
    "\n",
    "pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reload data that was processed last time\n",
    "pickle_file = 'data/training_data.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    \n",
    "    X_small = save['X_small']\n",
    "    y_small = save['y_small']\n",
    "    X_word_to_id = save['X_word_to_id']\n",
    "    X_id_to_word = save['X_id_to_word']\n",
    "    y_word_to_id = save['y_word_to_id']\n",
    "    y_id_to_word = save['y_id_to_word']\n",
    "    X_id_padded = save['X_id_padded']\n",
    "    y_id_padded = save['y_id_padded']\n",
    "    X_embeddings = save['X_embeddings']\n",
    "    y_embeddings = save['y_embeddings']\n",
    "    \n",
    "    del save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_id_padded_train, X_id_padded_test, y_id_padded_train, y_id_padded_test = model_selection.train_test_split(\n",
    "    X_id_padded, y_id_padded, test_size=0.1, random_state=123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(63189, 30), (7022, 30), (63189, 30), (7022, 30)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.shape for e in (X_id_padded_train, X_id_padded_test, y_id_padded_train, y_id_padded_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is as if following the second world war belgium had made UNK degrelle a life senator in order to shield him from legal proceedings by that very act ZERO\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([X_id_to_word[i] for i in X_id_padded_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c'est comme si au lendemain de la deuxiÃ¨me guerre mondiale la belgique avait nommÃ© UNK UNK sÃ©nateur Ã  vie pour le soustraire par lÃ  mÃªme aux poursuites judiciaires ZERO ZERO\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([y_id_to_word[i] for i in y_id_padded_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20003, 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert pre-trained embeddings to a tensor\n",
    "X_embeddings = torch.FloatTensor(X_embeddings)\n",
    "y_embeddings = torch.FloatTensor(y_embeddings)\n",
    "\n",
    "if use_cuda:\n",
    "    X_embeddings = X_embeddings.cuda()\n",
    "    y_embeddings = y_embeddings.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a embedding layer initialized with pre-trained embedding matrix\n",
    "def create_embedding(init_embeddings, trainable=True):\n",
    "    vocab_size, embedding_size = init_embeddings.size()\n",
    "    embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "    embedding.load_state_dict({'weight': init_embeddings})\n",
    "    \n",
    "    if use_cuda:\n",
    "        embedding = embedding.cuda()\n",
    "    \n",
    "    if not trainable:\n",
    "        for param in embeddings.parameters(): \n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return embedding, vocab_size, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(20003, 200), 20003, 200)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dimensions\n",
    "create_embedding(X_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create encoder RNN using LSTM\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, init_embeddings, hidden_size, n_layers=2):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding, vocab_size, embedding_size = create_embedding(init_embeddings)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "    \n",
    "    def forward(self, input, states):\n",
    "        output, states = self.lstm(self.embedding(input), states)\n",
    "        return output, states\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        init_hidden_state = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "        init_cell_state = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "        \n",
    "        if use_cuda:\n",
    "            return (init_hidden_state.cuda(), init_cell_state.cuda())\n",
    "        else:\n",
    "            return (init_hidden_state, init_cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly initialized weight matrices\n",
    "def arr(*size):\n",
    "    return torch.randn(size) / math.sqrt(size[0])\n",
    "\n",
    "def param(*size):\n",
    "    if use_cuda:\n",
    "        return nn.Parameter(arr(*size)).cuda()\n",
    "    else:\n",
    "        return nn.Parameter(arr(*size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy style dot operation to multiply a 3D matrix with a 2D one\n",
    "# Based on https://discuss.pytorch.org/t/how-can-i-compute-3d-tensor-2d-tensor-multiplication/639/9\n",
    "def dot(X, Y):\n",
    "    return torch.bmm(X, Y.unsqueeze(0).expand(X.size(0), *Y.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$u^t_i = v^T tanh(W_1â² h_i + W_2â² d_t)$$\n",
    "$$a^t_i = softmax(u^t_i)$$\n",
    "$$d_t' = \\sum_i^{T_A} a^t_i h_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, init_embeddings, hidden_size, n_layers=2):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding, vocab_size, embedding_size = create_embedding(init_embeddings)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Define weights and intercepts used in paper 1412.7449\n",
    "        # to construct the allignment matrix: u^t_i = v^T tanh(W_1â² h_i + W_2â² d_t)\n",
    "        self.W1 = param(hidden_size, hidden_size)\n",
    "        self.W2 = param(hidden_size, hidden_size)\n",
    "        self.b = param(hidden_size)\n",
    "        self.v = param(hidden_size)\n",
    "        \n",
    "        # Linear layer to reshape hidden state, concatenated with either the previous true label or prediction,\n",
    "        # back to the shape of hidden state\n",
    "        # As the new input to LSTM\n",
    "        self.new_input = nn.Linear(hidden_size + embedding_size, hidden_size)\n",
    "        \n",
    "        # LSTM layers using the new concatenated hidden state as the input\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "        # Linear layer to reshape data to the shape of output vocabulary\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.new_input = self.new_input.cuda()\n",
    "            self.lstm = self.lstm.cuda()\n",
    "            self.out = self.out.cuda()\n",
    "    \n",
    "    def forward(self, input, states, encoder_outputs):\n",
    "        # u^t_i = v^T tanh(W_1â² h_i + W_2â² d_t)\n",
    "        W1h = dot(encoder_outputs, self.W1)            # (batch_size, seq_length, hidden_size)\n",
    "        hidden_state = states[0]                       # (n_layers, batch_size, hidden_size)\n",
    "        W2d = hidden_state[-1].mm(self.W2)             # (batch_size, hidden_size)\n",
    "        W1h_W2d = W1h + W2d.unsqueeze(1) + self.b      # (batch_size, seq_length, hidden_size)\n",
    "        tahn_W1h_W2d = F.tanh(W1h_W2d)                 # (batch_size, seq_length, hidden_size)\n",
    "        u = (tahn_W1h_W2d * self.v).sum(2)             # (batch_size, seq_length)\n",
    "        \n",
    "        # a^t_i = softmax(u^t_i)\n",
    "        a = F.softmax(u)                               # (batch_size, seq_length)\n",
    "        \n",
    "        # d_t' = \\sum_i^{T_A} a^t_i h_i\n",
    "        encoder_outputs_weighted_sum = (a.unsqueeze(2) * encoder_outputs).sum(1)\n",
    "                                                       # (batch_size, hidden_size)\n",
    "        \n",
    "        # Concatenate with decoder input,\n",
    "        # which is either the previous true label or prediction\n",
    "        concat_input = torch.cat((encoder_outputs_weighted_sum, self.embedding(input)), 1)\n",
    "                                                       # (batch_size, hidden_size + embedding_size)\n",
    "        \n",
    "        # Reshape the concatenated input back to the shape of hidden state\n",
    "        reshaped_input = self.new_input(concat_input)  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Feed the new input into the LSTM layer\n",
    "        output, states = self.lstm(reshaped_input.unsqueeze(0), states)\n",
    "        output = output.squeeze(0)                     # (batch_size, hidden_size)\n",
    "        \n",
    "        # Finally, feed to the output layer\n",
    "        output = self.out(output)                      # (batch_size, vocab_size)\n",
    "        output = F.log_softmax(output)                 # (batch_size, vocab_size)\n",
    "        \n",
    "        return output, states, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X_input, y_input, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion, teacher_forcing_prob=0.5):\n",
    "    # Initialize variables\n",
    "    batch_size, X_seq_length = X_input.size()\n",
    "    y_seq_length = y_input.size()[1]\n",
    "    \n",
    "    encoder_states = encoder.initHidden(batch_size)\n",
    "    decoder_input = Variable(torch.LongTensor([X_word_to_id['GO']] * batch_size))\n",
    "    if use_cuda:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    # Encode\n",
    "    encoder_outputs, encoder_states = encoder(X_input, encoder_states)\n",
    "    decoder_states = encoder_states\n",
    "\n",
    "    # Decode\n",
    "    if np.random.random() <= teacher_forcing_prob:\n",
    "        # Teacher forcing: use the true label as the next decoder input\n",
    "        for i in range(y_seq_length):\n",
    "            decoder_output, decoder_states, decoder_attention = decoder(decoder_input, decoder_states, encoder_outputs)\n",
    "            loss += criterion(decoder_output, y_input[:, i])\n",
    "            decoder_input = y_input[:, i]\n",
    "    else:\n",
    "        # Otherwise, use the previous prediction\n",
    "        for i in range(y_seq_length):\n",
    "            decoder_output, decoder_states, decoder_attention = decoder(decoder_input, decoder_states, encoder_outputs)\n",
    "            loss += criterion(decoder_output, y_input[:, i])\n",
    "            \n",
    "            # Generate prediction\n",
    "            top_value, top_index = decoder_output.data.topk(1)\n",
    "            decoder_input = Variable(top_index.squeeze(1))\n",
    "            if use_cuda:\n",
    "                decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / y_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train an epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, i, batch_size):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    return X[start:end], y[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(X, y, batch_size, encoder, decoder, lr=0.01, teacher_forcing_prob=0.5):\n",
    "    total_loss = 0\n",
    "    \n",
    "    encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=lr)\n",
    "    decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    if use_cuda:\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    # loop over batches\n",
    "    epoch_size = len(X) // batch_size\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        X_batch, y_batch = get_batch(X, y, i, batch_size)\n",
    "        \n",
    "        X_batch = Variable(torch.from_numpy(X_batch).long())\n",
    "        y_batch = Variable(torch.from_numpy(y_batch).long())\n",
    "        \n",
    "        if use_cuda:\n",
    "            X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "        \n",
    "        loss = train(X_batch, y_batch, encoder, decoder, encoder_optimizer,\n",
    "                     decoder_optimizer, criterion, teacher_forcing_prob)\n",
    "        \n",
    "        total_loss += loss\n",
    "        \n",
    "    return total_loss / epoch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X_input, encoder, decoder, max_len):\n",
    "    # Initialize variables\n",
    "    batch_size, X_seq_length = X_input.size()\n",
    "    \n",
    "    encoder_states = encoder.initHidden(batch_size)\n",
    "    decoder_input = Variable(torch.LongTensor([X_word_to_id['GO']] * batch_size))\n",
    "    if use_cuda:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    # Encode\n",
    "    encoder_outputs, encoder_states = encoder(X_input, encoder_states)\n",
    "    decoder_states = encoder_states\n",
    "\n",
    "    # Decode\n",
    "    decoded_words = np.zeros((batch_size, max_len))\n",
    "    decoder_attentions = np.zeros((batch_size, max_len, max_len))\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        decoder_output, decoder_states, decoder_attention = decoder(decoder_input, decoder_states, encoder_outputs)\n",
    "        top_value, top_index = decoder_output.data.topk(1)\n",
    "        decoded_words[:, i] = top_index.squeeze(1).cpu().numpy()\n",
    "        decoder_attentions[:, i, :] = decoder_attention.data.cpu().numpy()\n",
    "        \n",
    "        # Use the prediction as the next decoder input\n",
    "        decoder_input = Variable(top_index.squeeze(1))\n",
    "        if use_cuda:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    return decoded_words, decoder_attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "max_len = len(X_id_padded[0])\n",
    "batch_size = 100\n",
    "hidden_size = 1000\n",
    "learning_rate = 0.005\n",
    "teacher_forcing_prob = 0.5\n",
    "\n",
    "encoder = EncoderRNN(X_embeddings, hidden_size)\n",
    "decoder = AttnDecoderRNN(y_embeddings, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(EncoderRNN (\n",
       "   (embedding): Embedding(20003, 200)\n",
       "   (lstm): LSTM(200, 1000, num_layers=2, batch_first=True)\n",
       " ), AttnDecoderRNN (\n",
       "   (embedding): Embedding(20003, 200)\n",
       "   (new_input): Linear (1200 -> 1000)\n",
       "   (lstm): LSTM(1000, 1000, num_layers=2)\n",
       "   (out): Linear (1000 -> 20003)\n",
       " ))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly pick sentences that are shorter than 10 words for testing\n",
    "X_ix = [i for i, e in enumerate(X_id_padded_test) if np.count_nonzero(e) <= 10 and X_word_to_id['UNK'] not in e]\n",
    "y_ix = [i for i, e in enumerate(y_id_padded_test) if np.count_nonzero(e) <= 10 and y_word_to_id['UNK'] not in e]\n",
    "ix = list(set(X_ix).intersection(y_ix))\n",
    "\n",
    "np.random.seed(123456)\n",
    "np.random.shuffle(ix)\n",
    "ix = ix[:3]\n",
    "\n",
    "X_test = X_id_padded_test[ix]\n",
    "y_test = y_id_padded_test[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.5/dist-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type AttnDecoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# Redirect all output to a file\n",
    "# First, save the default output\n",
    "orig_stdout = sys.stdout\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Redirect output to a file\n",
    "    log_file_path = 'log_file_epoch_' + str(i) + '.txt'\n",
    "    f = open(os.path.join('output', log_file_path), 'w')\n",
    "    sys.stdout = f\n",
    "    \n",
    "    print('Epoch:', i)\n",
    "    \n",
    "    # Shuffle the training data every epoch to avoid local minima\n",
    "    np.random.seed(i)\n",
    "    ix = np.arange(len(X_id_padded_train))\n",
    "    np.random.shuffle(ix)\n",
    "    \n",
    "    X_id_padded_train, y_id_padded_train = X_id_padded_train[ix], y_id_padded_train[ix]\n",
    "    \n",
    "    # Print out the first sentence in X and y for sanity check\n",
    "    print('First sentence in English:', ' '.join([X_id_to_word[ix] for ix in X_id_padded_train[0] if ix > 0]))\n",
    "    print('First sentence in French:', ' '.join([y_id_to_word[ix] for ix in y_id_padded_train[0] if ix > 0]))    \n",
    "    \n",
    "    # Train an epoch    \n",
    "    train_loss = train_epoch(X_id_padded_train, y_id_padded_train, batch_size,\n",
    "                             encoder, decoder, learning_rate, teacher_forcing_prob)\n",
    "    \n",
    "    print('\\nTraining loss:', train_loss)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    torch.save(encoder, 'output/encoder_' + str(i))\n",
    "    torch.save(decoder, 'output/decoder_' + str(i))\n",
    "    \n",
    "    # Evaluate\n",
    "    # Translate test sentences\n",
    "    X_test_var = Variable(torch.from_numpy(X_test).long())\n",
    "    if use_cuda:\n",
    "        X_test_var = X_test_var.cuda()\n",
    "    translations, decoder_attentions = evaluate(X_test_var, encoder, decoder, max_len)\n",
    "    \n",
    "    for t in range(X_test.shape[0]):\n",
    "        input_words = ' '.join([X_id_to_word[ix] for ix in X_test[t] if ix > 0])\n",
    "        target_words = ' '.join([y_id_to_word[ix] for ix in y_test[t] if ix > 0])\n",
    "        \n",
    "        # Cut off translations at the first 'ZERO' padding\n",
    "        first_zero_ix = np.where(translations[t] == 0)[0]\n",
    "        if len(first_zero_ix) > 0:\n",
    "            output_words = ' '.join([y_id_to_word[ix] for ix in translations[t][:first_zero_ix[0]]])\n",
    "        else:\n",
    "            output_words = ' '.join([y_id_to_word[ix] for ix in translations[t]])\n",
    "        \n",
    "        print('\\nTranslation of', input_words, ':', output_words)\n",
    "        print('Actual translation:', target_words)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "# Restore default output\n",
    "sys.stdout = orig_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f78e6440940>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f78e6254898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(decoder_attentions[0], cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "sys.stdout = orig_stdout\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = torch.load('output/encoder_29')\n",
    "decoder = torch.load('output/decoder_29')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(EncoderRNN (\n",
       "   (embedding): Embedding(20003, 200)\n",
       "   (lstm): LSTM(200, 256, num_layers=2, batch_first=True)\n",
       " ), AttnDecoderRNN (\n",
       "   (embedding): Embedding(20003, 200)\n",
       "   (new_input): Linear (456 -> 256)\n",
       "   (lstm): LSTM(256, 256, num_layers=2)\n",
       "   (out): Linear (256 -> 20003)\n",
       " ))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translation of on this the commission is determined and optimistic : en effet de de\n",
      "Actual translation: Ã  cet Ã©gard la commission est dÃ©terminÃ©e et optimiste\n",
      "\n",
      "Translation of we must take this phenomenon into account : nous est de de\n",
      "Actual translation: nous devons prendre en compte ce phÃ©nomÃ¨ne\n",
      "\n",
      "Translation of my final task as rapporteur is a thankful one : il est de de\n",
      "Actual translation: ma derniÃ¨re tÃ¢che de rapporteur est dâexprimer mes remerciements\n"
     ]
    }
   ],
   "source": [
    "X_test_var = Variable(torch.from_numpy(X_test).long())\n",
    "if use_cuda:\n",
    "    X_test_var = X_test_var.cuda()\n",
    "translations = evaluate(X_test_var, encoder, decoder)\n",
    "\n",
    "for t in range(X_test.shape[0]):\n",
    "    input_words = ' '.join([X_id_to_word[ix] for ix in X_test[t] if ix > 0])\n",
    "    target_words = ' '.join([y_id_to_word[ix] for ix in y_test[t] if ix > 0])\n",
    "\n",
    "    # Cut off translations at the first 'ZERO' padding\n",
    "    first_zero_ix = np.where(translations[t] == 0)[0]\n",
    "    if len(first_zero_ix) > 0:\n",
    "        output_words = ' '.join([y_id_to_word[ix] for ix in translations[t][:first_zero_ix[0]]])\n",
    "    else:\n",
    "        output_words = ' '.join([y_id_to_word[ix] for ix in translations[t]])\n",
    "\n",
    "    print('\\nTranslation of', input_words, ':', output_words)\n",
    "    print('Actual translation:', target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
