{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import math\n",
    "from gensim.models import word2vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previously processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload data that was processed last time\n",
    "pickle_file = 'data/training_data.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    \n",
    "    X_small = save['X_small']\n",
    "    y_small = save['y_small']\n",
    "    \n",
    "    del save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we need a new initiative from the commission on this\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(word for word in X_small[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il nous faut une nouvelle initiative de la commission à ce sujet\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(word for word in y_small[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word-to-index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_to_id_mapping(data, max_vocab_size = 20000):\n",
    "    counter = collections.Counter(np.hstack(data))\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    # Pick the most common ones\n",
    "    count_pairs = count_pairs[:max_vocab_size]\n",
    "\n",
    "    # Add 'ZERO', 'GO', and 'UNK'\n",
    "    # It is important to add 'ZERO' in the beginning\n",
    "    # to make sure zero padding does not interfere with existing words\n",
    "    count_pairs.insert(0, ('GO', 0))\n",
    "    count_pairs.insert(0, ('ZERO', 0))\n",
    "    count_pairs.append(('UNK', 0))\n",
    "\n",
    "    # Create mapping for both directions\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    id_to_word = dict(zip(range(len(words)), words))\n",
    "    \n",
    "    # Map words to indexes\n",
    "    data_id = [[word_to_id[word] if word in word_to_id else word_to_id['UNK'] for word in sentence] for sentence in data]\n",
    "    \n",
    "    return word_to_id, id_to_word, data_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_to_id, X_id_to_word, X_id = create_word_to_id_mapping(X_small)\n",
    "y_word_to_id, y_id_to_word, y_id = create_word_to_id_mapping(y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000 20003 20003\n"
     ]
    }
   ],
   "source": [
    "print(len(X_id), len(y_id), len(X_word_to_id), len(y_word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vocab_size, y_vocab_size = len(X_word_to_id), len(y_word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we need a new initiative from the commission on this\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([X_id_to_word[i] for i in X_id[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il nous faut une nouvelle initiative de la commission à ce sujet\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([y_id_to_word[i] for i in y_id[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad zeros to make sentences equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50  # As defined last time\n",
    "X_id_padded = pad_sequences(X_id, maxlen=max_len, padding='post')\n",
    "y_id_padded = pad_sequences(y_id, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverage pre-trained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English word vectors downloaded from https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code stolen from https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "embeddings_index_en = {}\n",
    "f = open('data/glove.6B/glove.6B.200d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index_en[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French word vectors downloaded from http://fauconnier.github.io/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index_fr = word2vec.KeyedVectors.load_word2vec_format(\n",
    "    'data/frWac_non_lem_no_postag_no_phrase_200_skip_cut100.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map words to pre-trained embeddings\n",
    "def map_word_to_pretrained_embedding(embeddings_index, embeddings_dim, word_to_id):\n",
    "    vocab_size = len(word_to_id)\n",
    "    embedding_matrix = np.zeros((vocab_size, embeddings_dim))\n",
    "    \n",
    "    # Keep a running count of matched words\n",
    "    found = 0\n",
    "    \n",
    "    for word, i in word_to_id.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            found += 1\n",
    "        else:\n",
    "            # Words not found in embedding index will be randomly initialized\n",
    "            embedding_matrix[i] = np.random.normal(size=(embedding_size, ))\n",
    "\n",
    "    return embedding_matrix, found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20003, 200), 18053)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embeddings, X_found = map_word_to_pretrained_embedding(embeddings_index_en, 200, X_word_to_id)\n",
    "X_embeddings.shape, X_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20003, 200), 17287)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_embeddings, y_found = map_word_to_pretrained_embedding(embeddings_index_fr, 200, y_word_to_id)\n",
    "y_embeddings.shape, y_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'data/training_data.pickle'\n",
    "\n",
    "f = open(pickle_file, 'wb')\n",
    "save = {\n",
    "    'X_small': X_small,\n",
    "    'y_small': y_small,\n",
    "    'X_word_to_id': X_word_to_id,\n",
    "    'X_id_to_word': X_id_to_word,\n",
    "    'y_word_to_id': y_word_to_id,\n",
    "    'y_id_to_word': y_id_to_word,\n",
    "    'X_id_padded': X_id_padded,\n",
    "    'y_id_padded': y_id_padded,\n",
    "    'X_embeddings': X_embeddings,\n",
    "    'y_embeddings': y_embeddings\n",
    "}\n",
    "\n",
    "pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reload data that was processed last time\n",
    "pickle_file = 'data/training_data.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    \n",
    "    X_small = save['X_small']\n",
    "    y_small = save['y_small']\n",
    "    X_word_to_id = save['X_word_to_id']\n",
    "    X_id_to_word = save['X_id_to_word']\n",
    "    y_word_to_id = save['y_word_to_id']\n",
    "    y_id_to_word = save['y_id_to_word']\n",
    "    X_id_padded = save['X_id_padded']\n",
    "    y_id_padded = save['y_id_padded']\n",
    "    X_embeddings = save['X_embeddings']\n",
    "    y_embeddings = save['y_embeddings']\n",
    "    \n",
    "    del save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_id_padded_train, X_id_padded_test, y_id_padded_train, y_id_padded_test = model_selection.train_test_split(\n",
    "    X_id_padded, y_id_padded, test_size=0.1, random_state=123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(90000, 50), (10000, 50), (90000, 50), (10000, 50)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.shape for e in (X_id_padded_train, X_id_padded_test, y_id_padded_train, y_id_padded_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i would like to draw attention to one of the commission's most important commitments to reduce poverty in europe and increase social inclusion ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([X_id_to_word[i] for i in X_id_padded_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je souhaite attirer votre attention sur un des engagements les plus importants de la commission à savoir la réduction de la pauvreté en europe et le renforcement de l'inclusion sociale ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([y_id_to_word[i] for i in y_id_padded_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20003, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a embedding layer initialized with pre-trained embedding matrix\n",
    "def create_embedding(init_embeddings, trainable=True):\n",
    "    vocab_size, embedding_size = init_embeddings.shape\n",
    "    embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "    \n",
    "    # Convert pre-trained embeddings to a tensor\n",
    "    if use_cuda:\n",
    "        init_embeddings = torch.FloatTensor(init_embeddings).cuda()\n",
    "    else:\n",
    "        init_embeddings = torch.FloatTensor(init_embeddings)\n",
    "    \n",
    "    embedding.load_state_dict({'weight': init_embeddings})\n",
    "    \n",
    "    if not trainable:\n",
    "        for param in embeddings.parameters(): \n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return embedding, vocab_size, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(20003, 200), 20003, 200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dimensions\n",
    "create_embedding(X_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create encoder RNN using LSTM\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, init_embeddings, hidden_size, n_layers=2):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding, vocab_size, embedding_size = create_embedding(init_embeddings)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, input, states):\n",
    "        output, states = self.lstm(self.embedding(input), states)\n",
    "        return output, states\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        init_hidden_state = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "        init_cell_state = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "        return (init_hidden_state, init_cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly initialized weight matrices\n",
    "def arr(*size):\n",
    "    return torch.randn(size) / math.sqrt(size[0])\n",
    "\n",
    "def param(*size):\n",
    "    if use_cuda:\n",
    "        return nn.Parameter(arr(*size)).cuda()\n",
    "    else:\n",
    "        return nn.Parameter(arr(*size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy style dot operation to multiply a 3D matrix with a 2D one\n",
    "# Based on https://discuss.pytorch.org/t/how-can-i-compute-3d-tensor-2d-tensor-multiplication/639/9\n",
    "def dot(X, Y):\n",
    "    return torch.bmm(X, Y.unsqueeze(0).expand(X.size(0), *Y.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$u^t_i = v^T tanh(W_1′ h_i + W_2′ d_t)$$\n",
    "$$a^t_i = softmax(u^t_i)$$\n",
    "$$d_t' = \\sum_i^{T_A} a^t_i h_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, init_embeddings, hidden_size, n_layers=2):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding, vocab_size, embedding_size = create_embedding(init_embeddings)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Define weights and intercepts used in paper 1412.7449\n",
    "        # to construct the allignment matrix: u^t_i = v^T tanh(W_1′ h_i + W_2′ d_t)\n",
    "        self.W1 = param(hidden_size, hidden_size)\n",
    "        self.W2 = param(hidden_size, hidden_size)\n",
    "        self.b = param(hidden_size)\n",
    "        self.v = param(hidden_size)\n",
    "        \n",
    "        # Linear layer to reshape hidden state, concatenated with either the previous true label or prediction,\n",
    "        # back to the shape of hidden state\n",
    "        # As the new input to LSTM\n",
    "        self.new_input = nn.Linear(hidden_size + embedding_size, hidden_size)\n",
    "        \n",
    "        # LSTM layers using the new concatenated hidden state as the input\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "        # Linear layer to reshape data to the shape of output vocabulary\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input, states, encoder_outputs):\n",
    "        # u^t_i = v^T tanh(W_1′ h_i + W_2′ d_t)\n",
    "        W1h = dot(encoder_outputs, self.W1)            # (batch_size, seq_length, hidden_size)\n",
    "        hidden_state = states[0]                       # (n_layers, batch_size, hidden_size)\n",
    "        W2d = hidden_state[-1].mm(self.W2)             # (batch_size, hidden_size)\n",
    "        W1h_W2d = W1h + W2d.unsqueeze(1) + self.b      # (batch_size, seq_length, hidden_size)\n",
    "        tahn_W1h_W2d = F.tanh(W1h_W2d)                 # (batch_size, seq_length, hidden_size)\n",
    "        u = (tahn_W1h_W2d * self.v).sum(2)             # (batch_size, seq_length)\n",
    "        \n",
    "        # a^t_i = softmax(u^t_i)\n",
    "        a = F.softmax(u)                               # (batch_size, seq_length)\n",
    "        \n",
    "        # d_t' = \\sum_i^{T_A} a^t_i h_i\n",
    "        weighted_encoder_outputs = (a.unsqueeze(2) * encoder_outputs).sum(1)\n",
    "                                                       # (batch_size, hidden_size)\n",
    "        \n",
    "        # Concatenate with decoder input,\n",
    "        # which is either the previous true label or prediction\n",
    "        concat_input = torch.cat((weighted_encoder_outputs, self.embedding(input)), 1)\n",
    "                                                       # (batch_size, hidden_size + embedding_size)\n",
    "        \n",
    "        # Reshape the concatenated input back to the shape of hidden state\n",
    "        reshaped_input = self.new_input(concat_input)  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Feed the new input into the LSTM layer\n",
    "        output, states = self.lstm(reshaped_input.unsqueeze(0), states)\n",
    "        output = output.squeeze(0)                     # (batch_size, hidden_size)\n",
    "        \n",
    "        # Finally, feed to the output layer\n",
    "        output = self.out(output)                      # (batch_size, vocab_size)\n",
    "        \n",
    "        return output, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X_input, y_input, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion, teacher_forcing_prob):\n",
    "    # Initialize variables\n",
    "    batch_size, X_seq_length = X_input.size()\n",
    "    y_seq_length = y_input.size()[1]\n",
    "    \n",
    "    if use_cuda:\n",
    "        encoder_states = encoder.initHidden(batch_size).cuda()\n",
    "    else:\n",
    "        encoder_states = encoder.initHidden(batch_size)\n",
    "    \n",
    "    decoder_input = Variable(torch.LongTensor([X_word_to_id['GO']] * batch_size))\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    # Encode\n",
    "    encoder_outputs, encoder_states = encoder(X_input, encoder_states)\n",
    "    decoder_states = encoder_states\n",
    "\n",
    "    # Decode\n",
    "    used_teacher_forcing = 0\n",
    "    if np.random.random() <= teacher_forcing_prob:\n",
    "        # Teacher forcing: use the true label as the next decoder input\n",
    "        used_teacher_forcing = 1\n",
    "        \n",
    "        for i in range(y_seq_length):\n",
    "            decoder_output, decoder_states = decoder(decoder_input, decoder_states, encoder_outputs)\n",
    "            loss += criterion(decoder_output, y_input[:, i])\n",
    "            decoder_input = y_input[:, i]\n",
    "    else:\n",
    "        # Otherwise, use the previous prediction\n",
    "        for i in range(y_seq_length):\n",
    "            decoder_output, decoder_states = decoder(decoder_input, decoder_states, encoder_outputs)\n",
    "            loss += criterion(decoder_output, y_input[:, i])\n",
    "            \n",
    "            # Generate prediction\n",
    "            top_value, top_index = decoder_output.data.topk(1)\n",
    "            decoder_input = Variable(top_index.squeeze(1))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / y_seq_length, used_teacher_forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train an epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, i, batch_size):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    return X[start:end], y[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(X, y, batch_size, encoder, decoder, lr=0.01, teacher_forcing_prob=1):\n",
    "    total_loss = 0\n",
    "    \n",
    "    encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=lr)\n",
    "    decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=lr)\n",
    "    \n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    \n",
    "    # loop over batches\n",
    "    epoch_size = len(X) // batch_size\n",
    "    times_used_teacher_forcing = 0\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        X_batch, y_batch = get_batch(X, y, i, batch_size)\n",
    "        \n",
    "        X_batch = Variable(torch.from_numpy(X_batch).long())\n",
    "        y_batch = Variable(torch.from_numpy(y_batch).long())\n",
    "        \n",
    "        loss, used_teacher_forcing = train(X_batch, y_batch, encoder, decoder, encoder_optimizer,\n",
    "                                           decoder_optimizer, criterion, teacher_forcing_prob)\n",
    "        \n",
    "        total_loss += loss\n",
    "        times_used_teacher_forcing += used_teacher_forcing\n",
    "    \n",
    "    return total_loss / epoch_size, times_used_teacher_forcing / epoch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X_input, encoder, decoder):\n",
    "    # Initialize variables\n",
    "    batch_size, X_seq_length = X_input.size()\n",
    "    \n",
    "    if use_cuda:\n",
    "        encoder_states = encoder.initHidden(batch_size).cuda()\n",
    "    else:\n",
    "        encoder_states = encoder.initHidden(batch_size)\n",
    "    \n",
    "    decoder_input = Variable(torch.LongTensor([X_word_to_id['GO']] * batch_size))\n",
    "\n",
    "    # Encode\n",
    "    encoder_outputs, encoder_states = encoder(X_input, encoder_states)\n",
    "    decoder_states = encoder_states\n",
    "\n",
    "    # Decode\n",
    "    decoded_words = np.zeros((batch_size, 50))\n",
    "    for i in range(max_len):\n",
    "        decoder_output, decoder_states = decoder(decoder_input, decoder_states, encoder_outputs)\n",
    "        top_value, top_index = decoder_output.data.topk(1)\n",
    "        decoded_words[:, i] = top_index.squeeze(1).numpy()\n",
    "        \n",
    "        # Use the prediction as the next decoder input\n",
    "        decoder_input = Variable(top_index.squeeze(1))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "max_len = 50\n",
    "batch_size = 100\n",
    "hidden_size = 256\n",
    "learning_rate = 0.01\n",
    "\n",
    "encoder = EncoderRNN(X_embeddings, hidden_size)\n",
    "decoder = AttnDecoderRNN(y_embeddings, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly pick sentences that are shorter than 10 words for testing\n",
    "X_ix = [i for i, e in enumerate(X_id_padded_test) if np.count_nonzero(e) <= 10 and X_word_to_id['UNK'] not in e]\n",
    "y_ix = [i for i, e in enumerate(y_id_padded_test) if np.count_nonzero(e) <= 10 and y_word_to_id['UNK'] not in e]\n",
    "ix = list(set(X_ix).intersection(y_ix))\n",
    "\n",
    "np.random.seed(123456)\n",
    "np.random.shuffle(ix)\n",
    "ix = ix[:3]\n",
    "\n",
    "X_test = X_id_padded_test[ix]\n",
    "y_test = y_id_padded_test[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "First sentence in English: the resolution condemns the israeli authorities for using inappropriate methods to deal with minors\n",
      "First sentence in French: la résolution condamne les autorités israéliennes qu'elle accuse d'utiliser des méthodes inappropriées face à des mineurs\n"
     ]
    }
   ],
   "source": [
    "teacher_forcing_prob = 1  # initial ratio\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('Epoch:', i)\n",
    "    \n",
    "    # Shuffle the training data every epoch to avoid local minima\n",
    "    np.random.seed(i)\n",
    "    ix = np.arange(len(X_id_padded_train))\n",
    "    np.random.shuffle(ix)\n",
    "    \n",
    "    X_id_padded_train, y_id_padded_train = X_id_padded_train[ix], y_id_padded_train[ix]\n",
    "    \n",
    "    # Print out the first sentence in X and y for sanity check\n",
    "    print('First sentence in English:', ' '.join([X_id_to_word[ix] for ix in X_id_padded_train[0] if ix > 0]))\n",
    "    print('First sentence in French:', ' '.join([y_id_to_word[ix] for ix in y_id_padded_train[0] if ix > 0]))    \n",
    "    \n",
    "    # Train an epoch    \n",
    "    train_loss, times_used_teacher_forcing = train_epoch(X_id_padded_train, y_id_padded_train, batch_size,\n",
    "                                                         encoder, decoder, learning_rate, teacher_forcing_prob)\n",
    "    \n",
    "    # Gradually decrease teacher_forcing_prob\n",
    "    teacher_forcing_prob -= 1 / epochs\n",
    "    \n",
    "    print('\\nTraining loss:', train_loss)\n",
    "    print('teacher_forcing_prob =', round(teacher_forcing_prob, 2), 'times_used_teacher_forcing =', round(times_used_teacher_forcing, 2))\n",
    "    \n",
    "    # Save checkpoint\n",
    "    torch.save(encoder, 'output/encoder_' + str(i))\n",
    "    torch.save(decoder, 'output/decoder_' + str(i))\n",
    "    \n",
    "    # Evaluate\n",
    "    # Translate test sentences\n",
    "    translations = evaluate(Variable(torch.from_numpy(X_test).long()), encoder, decoder)\n",
    "    \n",
    "    for t in range(X_test.shape[0]):\n",
    "        input_words = ' '.join([X_id_to_word[ix] for ix in X_test[t] if ix > 0])\n",
    "        target_words = ' '.join([y_id_to_word[ix] for ix in y_test[t] if ix > 0])\n",
    "        \n",
    "        # Cut off translations at the first 'ZERO' padding\n",
    "        first_zero_ix = np.where(translations[t] == 0)[0]\n",
    "        if len(first_zero_ix) > 0:\n",
    "            output_words = ' '.join([y_id_to_word[ix] for ix in translations[t][:first_zero_ix[0]]])\n",
    "        else:\n",
    "            output_words = ' '.join([y_id_to_word[ix] for ix in translations[t]])\n",
    "        \n",
    "        print('\\nTranslation of', input_words, ':', output_words)\n",
    "        print('Actual translation:', target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.load('output/encoder_6')\n",
    "decoder = torch.load('output/decoder_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(EncoderRNN (\n",
       "   (embedding): Embedding(20003, 200)\n",
       "   (lstm): LSTM(200, 200, num_layers=2, batch_first=True)\n",
       " ), AttnDecoderRNN (\n",
       "   (embedding): Embedding(20003, 200)\n",
       "   (new_input): Linear (400 -> 200)\n",
       "   (lstm): LSTM(200, 200, num_layers=2)\n",
       "   (out): Linear (200 -> 20003)\n",
       " ))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "translations = evaluate(Variable(torch.from_numpy(X_test).long()), encoder, decoder)\n",
    "\n",
    "for t in range(X_test.shape[0]):\n",
    "    input_words = ' '.join([X_id_to_word[ix] for ix in X_test[t] if ix > 0])\n",
    "    target_words = ' '.join([y_id_to_word[ix] for ix in y_test[t] if ix > 0])\n",
    "\n",
    "    # Cut off translations at the first 'ZERO' padding\n",
    "    first_zero_ix = np.where(translations[t] == 0)[0]\n",
    "    if len(first_zero_ix) > 0:\n",
    "        output_words = ' '.join([y_id_to_word[ix] for ix in translations[t][:first_zero_ix[0]]])\n",
    "    else:\n",
    "        output_words = ' '.join([y_id_to_word[ix] for ix in translations[t]])\n",
    "\n",
    "    print('\\nTranslation of', input_words, ':', output_words)\n",
    "    print('Actual translation:', target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
