{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import model_selection\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#loading-data-files\n",
    "lines = open('data/tatoeba/eng-fra.txt').read().strip().split('\\n')\n",
    "pairs = [l.split('\\t') for l in lines]\n",
    "pairs = [[text_to_word_sequence(re.sub(r\"\\u202f|\\u2009\", r\"\", s)) for s in p] for p in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [p[0] for p in pairs]\n",
    "y = [p[1] for p in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9962824457826004"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For ease of training, only keep sentences shorter than 20 words\n",
    "X_len = [len(sentence) for sentence in X]\n",
    "y_len = [len(sentence) for sentence in y]\n",
    "\n",
    "min_len = 2\n",
    "max_len = 20\n",
    "\n",
    "X_to_keep_ix = np.where((np.array(X_len) >= min_len) & (np.array(X_len) <= max_len))\n",
    "y_to_keep_ix = np.where((np.array(y_len) >= min_len) & (np.array(y_len) <= max_len))\n",
    "\n",
    "to_keep_ix = list(set(np.intersect1d(X_to_keep_ix, y_to_keep_ix)))\n",
    "len(to_keep_ix) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135337, 135337)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_small = np.array(X)[to_keep_ix]\n",
    "y_small = np.array(y)[to_keep_ix]\n",
    "\n",
    "len(X_small), len(y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 20, 2, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "X_len = [len(sentence) for sentence in X_small]\n",
    "y_len = [len(sentence) for sentence in y_small]\n",
    "\n",
    "min(X_len), max(X_len), min(y_len), max(y_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([list(['i', 'see']), list(['i', 'won']), list(['i', 'won']),\n",
       "        list(['oh', 'no']), list(['get', 'up'])], dtype=object),\n",
       " array([list(['je', 'comprends']), list([\"j'ai\", 'gagné']),\n",
       "        list(['je', \"l'ai\", 'emporté']), list(['oh', 'non']),\n",
       "        list(['lève', 'toi'])], dtype=object))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_small[:5], y_small[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word-to-index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word_to_id_mapping(data, max_vocab_size = 20000):\n",
    "    counter = collections.Counter(np.hstack(data))\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    # Pick the most common ones\n",
    "    count_pairs = count_pairs[:max_vocab_size]\n",
    "\n",
    "    # Add 'ZERO', 'GO', and 'UNK'\n",
    "    # It is important to add 'ZERO' in the beginning\n",
    "    # to make sure zero padding does not interfere with existing words\n",
    "    count_pairs.insert(0, ('GO', 0))\n",
    "    count_pairs.insert(0, ('ZERO', 0))\n",
    "    count_pairs.append(('UNK', 0))\n",
    "\n",
    "    # Create mapping for both directions\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    id_to_word = dict(zip(range(len(words)), words))\n",
    "    \n",
    "    # Map words to indexes\n",
    "    data_id = [[word_to_id[word] if word in word_to_id else word_to_id['UNK'] for word in sentence] for sentence in data]\n",
    "    \n",
    "    return word_to_id, id_to_word, data_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_word_to_id, X_id_to_word, X_id = create_word_to_id_mapping(X_small)\n",
    "y_word_to_id, y_id_to_word, y_id = create_word_to_id_mapping(y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135337 135337 13512 20003\n"
     ]
    }
   ],
   "source": [
    "print(len(X_id), len(y_id), len(X_word_to_id), len(y_word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_vocab_size, y_vocab_size = len(X_word_to_id), len(y_word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i see\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([X_id_to_word[i] for i in X_id[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je comprends\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([y_id_to_word[i] for i in y_id[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad zeros to make sentences equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_id_padded = pad_sequences(X_id, maxlen=max_len, padding='post')\n",
    "y_id_padded = pad_sequences(y_id, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 20, 20)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "X_len = [len(sentence) for sentence in X_id_padded]\n",
    "y_len = [len(sentence) for sentence in y_id_padded]\n",
    "\n",
    "min(X_len), max(X_len), min(y_len), max(y_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverage pre-trained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English word vectors downloaded from https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code stolen from https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "embeddings_index_en = {}\n",
    "f = open('data/glove.6B/glove.6B.200d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index_en[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French word vectors downloaded from http://fauconnier.github.io/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index_fr = word2vec.KeyedVectors.load_word2vec_format(\n",
    "    'data/frWac_non_lem_no_postag_no_phrase_200_skip_cut100.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map words to pre-trained embeddings\n",
    "def map_word_to_pretrained_embedding(embeddings_index, embedding_size, word_to_id):\n",
    "    vocab_size = len(word_to_id)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "    \n",
    "    # Keep a running count of matched words\n",
    "    found = 0\n",
    "    \n",
    "    for word, i in word_to_id.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            found += 1\n",
    "        else:\n",
    "            # Words not found in embedding index will be randomly initialized\n",
    "            embedding_matrix[i] = np.random.normal(size=(embedding_size, ))\n",
    "\n",
    "    return embedding_matrix, found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13512, 200), 0.9655121373593842)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embeddings, X_found = map_word_to_pretrained_embedding(embeddings_index_en, 200, X_word_to_id)\n",
    "X_embeddings.shape, X_found / X_embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20003, 200), 0.83257511373294)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_embeddings, y_found = map_word_to_pretrained_embedding(embeddings_index_fr, 200, y_word_to_id)\n",
    "y_embeddings.shape, y_found / y_embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'data/tatoeba/training_data.pickle'\n",
    "\n",
    "f = open(pickle_file, 'wb')\n",
    "save = {\n",
    "    'X_small': X_small,\n",
    "    'y_small': y_small,\n",
    "    'X_word_to_id': X_word_to_id,\n",
    "    'X_id_to_word': X_id_to_word,\n",
    "    'y_word_to_id': y_word_to_id,\n",
    "    'y_id_to_word': y_id_to_word,\n",
    "    'X_id_padded': X_id_padded,\n",
    "    'y_id_padded': y_id_padded,\n",
    "    'X_embeddings': X_embeddings,\n",
    "    'y_embeddings': y_embeddings\n",
    "}\n",
    "\n",
    "pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reload data that was processed last time\n",
    "pickle_file = 'data/tatoeba/training_data.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    \n",
    "    X_small = save['X_small']\n",
    "    y_small = save['y_small']\n",
    "    X_word_to_id = save['X_word_to_id']\n",
    "    X_id_to_word = save['X_id_to_word']\n",
    "    y_word_to_id = save['y_word_to_id']\n",
    "    y_id_to_word = save['y_id_to_word']\n",
    "    X_id_padded = save['X_id_padded']\n",
    "    y_id_padded = save['y_id_padded']\n",
    "    X_embeddings = save['X_embeddings']\n",
    "    y_embeddings = save['y_embeddings']\n",
    "    \n",
    "    del save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51054, 51054)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing purposes, only include sentences that start with \"I\", \"you\", \"he\", \"she\", and \"we\"\n",
    "starts_with_subject_pronouns = np.array([\n",
    "    s[0] in [\n",
    "        X_word_to_id[\"i\"], X_word_to_id[\"you\"], X_word_to_id[\"he\"],\n",
    "        X_word_to_id[\"she\"], X_word_to_id[\"we\"]\n",
    "    ] for s in X_id_padded\n",
    "])\n",
    "starts_with_subject_pronouns = np.where(starts_with_subject_pronouns)[0]\n",
    "\n",
    "X_id_padded, y_id_padded = X_id_padded[starts_with_subject_pronouns], y_id_padded[starts_with_subject_pronouns]\n",
    "len(X_id_padded), len(y_id_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_id_padded_train, X_id_padded_test, y_id_padded_train, y_id_padded_test = model_selection.train_test_split(\n",
    "    X_id_padded, y_id_padded, test_size=0.1, random_state=123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(45948, 20), (5106, 20), (45948, 20), (5106, 20)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.shape for e in (X_id_padded_train, X_id_padded_test, y_id_padded_train, y_id_padded_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we want peace in the world ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([X_id_to_word[i] for i in X_id_padded_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nous désirons la paix dans le monde ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([y_id_to_word[i] for i in y_id_padded_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13512, 200)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert pre-trained embeddings to a tensor\n",
    "X_embeddings = torch.FloatTensor(X_embeddings)\n",
    "y_embeddings = torch.FloatTensor(y_embeddings)\n",
    "\n",
    "if use_cuda:\n",
    "    X_embeddings = X_embeddings.cuda()\n",
    "    y_embeddings = y_embeddings.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a embedding layer initialized with pre-trained embedding matrix\n",
    "def create_embedding(init_embeddings, trainable=True):\n",
    "    vocab_size, embedding_size = init_embeddings.size()\n",
    "    embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "    embedding.load_state_dict({'weight': init_embeddings})\n",
    "    \n",
    "    if use_cuda:\n",
    "        embedding = embedding.cuda()\n",
    "    \n",
    "    if not trainable:\n",
    "        for param in embeddings.parameters(): \n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return embedding, vocab_size, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(13512, 200), 13512, 200)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dimensions\n",
    "create_embedding(X_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create encoder RNN using LSTM\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, init_embeddings, hidden_size, n_layers=2):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding, vocab_size, embedding_size = create_embedding(init_embeddings)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "        if use_cuda:\n",
    "            self.lstm = self.lstm.cuda()\n",
    "    \n",
    "    def forward(self, input, states):\n",
    "        output, states = self.lstm(self.embedding(input), states)\n",
    "        return output, states\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        init_hidden_state = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "        init_cell_state = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "        \n",
    "        if use_cuda:\n",
    "            return (init_hidden_state.cuda(), init_cell_state.cuda())\n",
    "        else:\n",
    "            return (init_hidden_state, init_cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly initialized weight matrices\n",
    "def arr(*size):\n",
    "    return torch.randn(size) / math.sqrt(size[0])\n",
    "\n",
    "def param(*size):\n",
    "    if use_cuda:\n",
    "        return nn.Parameter(arr(*size)).cuda()\n",
    "    else:\n",
    "        return nn.Parameter(arr(*size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy style dot operation to multiply a 3D matrix with a 2D one\n",
    "# Based on https://discuss.pytorch.org/t/how-can-i-compute-3d-tensor-2d-tensor-multiplication/639/9\n",
    "def dot(X, Y):\n",
    "    return torch.bmm(X, Y.unsqueeze(0).expand(X.size(0), *Y.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$u^t_i = v^T tanh(W_1′ h_i + W_2′ d_t)$$\n",
    "$$a^t_i = softmax(u^t_i)$$\n",
    "$$d_t' = \\sum_i^{T_A} a^t_i h_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, init_embeddings, hidden_size, n_layers=2):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding, vocab_size, embedding_size = create_embedding(init_embeddings)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Define weights and intercepts used in paper 1412.7449\n",
    "        # to construct the allignment matrix: u^t_i = v^T tanh(W_1′ h_i + W_2′ d_t)\n",
    "        self.W1 = param(hidden_size, hidden_size)\n",
    "        self.W2 = param(hidden_size, hidden_size)\n",
    "        self.b = param(hidden_size)\n",
    "        self.v = param(hidden_size)\n",
    "        \n",
    "        # Linear layer to reshape hidden state, concatenated with either the previous true label or prediction,\n",
    "        # back to the shape of hidden state\n",
    "        # As the new input to LSTM\n",
    "        self.new_input = nn.Linear(hidden_size + embedding_size, hidden_size)\n",
    "        \n",
    "        # LSTM layers using the new concatenated hidden state as the input\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "        # Linear layer to reshape data to the shape of output vocabulary\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.new_input = self.new_input.cuda()\n",
    "            self.lstm = self.lstm.cuda()\n",
    "            self.out = self.out.cuda()\n",
    "    \n",
    "    def forward(self, input, states, encoder_outputs):\n",
    "        # u^t_i = v^T tanh(W_1′ h_i + W_2′ d_t)\n",
    "        W1h = dot(encoder_outputs, self.W1)            # (batch_size, seq_length, hidden_size)\n",
    "        hidden_state = states[0]                       # (n_layers, batch_size, hidden_size)\n",
    "        W2d = hidden_state[-1].mm(self.W2)             # (batch_size, hidden_size)\n",
    "        W1h_W2d = W1h + W2d.unsqueeze(1) + self.b      # (batch_size, seq_length, hidden_size)\n",
    "        tahn_W1h_W2d = F.tanh(W1h_W2d)                 # (batch_size, seq_length, hidden_size)\n",
    "        u = (tahn_W1h_W2d * self.v).sum(2)             # (batch_size, seq_length)\n",
    "        \n",
    "        # a^t_i = softmax(u^t_i)\n",
    "        a = F.softmax(u)                               # (batch_size, seq_length)\n",
    "        \n",
    "        # d_t' = \\sum_i^{T_A} a^t_i h_i\n",
    "        encoder_outputs_weighted_sum = (a.unsqueeze(2) * encoder_outputs).sum(1)\n",
    "                                                       # (batch_size, hidden_size)\n",
    "        \n",
    "        # Concatenate with decoder input,\n",
    "        # which is either the previous true label or prediction\n",
    "        concat_input = torch.cat((encoder_outputs_weighted_sum, self.embedding(input)), 1)\n",
    "                                                       # (batch_size, hidden_size + embedding_size)\n",
    "        \n",
    "        # Reshape the concatenated input back to the shape of hidden state\n",
    "        reshaped_input = self.new_input(concat_input)  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Feed the new input into the LSTM layer\n",
    "        output, states = self.lstm(reshaped_input.unsqueeze(0), states)\n",
    "        output = output.squeeze(0)                     # (batch_size, hidden_size)\n",
    "        \n",
    "        # Finally, feed to the output layer\n",
    "        output = self.out(output)                      # (batch_size, vocab_size)\n",
    "        output = F.log_softmax(output)                 # (batch_size, vocab_size)\n",
    "        \n",
    "        return output, states, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, i, batch_size):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    return X[start:end], y[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "i = 0\n",
    "\n",
    "X_input, y_input = get_batch(X_id_padded_train, y_id_padded_train, i, batch_size)\n",
    "X_input, y_input = Variable(torch.from_numpy(X_input).long()), Variable(torch.from_numpy(y_input).long())\n",
    "X_seq_length, y_seq_length = X_input.size()[1], y_input.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 10]), torch.Size([2, 5, 10]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "encoder = EncoderRNN(X_embeddings, hidden_size)\n",
    "\n",
    "encoder_states = encoder.initHidden(batch_size)\n",
    "encoder_states[0].size(), encoder_states[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 20, 10]), torch.Size([2, 5, 10]), torch.Size([2, 5, 10]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs, encoder_states = encoder(X_input, encoder_states)\n",
    "encoder_outputs.size(), encoder_states[0].size(), encoder_states[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = AttnDecoderRNN(y_embeddings, hidden_size)\n",
    "\n",
    "decoder_states = encoder_states\n",
    "decoder_input = Variable(torch.LongTensor([X_word_to_id['GO']] * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_attentions = np.zeros((batch_size, y_seq_length, y_seq_length))\n",
    "\n",
    "for i in range(y_seq_length):\n",
    "    decoder_output, decoder_states, decoder_attention = decoder(decoder_input, decoder_states, encoder_outputs)\n",
    "    decoder_input = y_input[:, i]\n",
    "    decoder_attentions[:, i, :] = decoder_attention.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 20003]),\n",
       " torch.Size([2, 5, 10]),\n",
       " torch.Size([2, 5, 10]),\n",
       " torch.Size([5, 20]),\n",
       " (5, 20, 20))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output.size(), decoder_states[0].size(), decoder_states[1].size(), decoder_attention.size(), decoder_attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X_input, y_input, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion, teacher_forcing_prob=0.5):\n",
    "    # Initialize variables\n",
    "    batch_size, X_seq_length = X_input.size()\n",
    "    y_seq_length = y_input.size()[1]\n",
    "    \n",
    "    encoder_states = encoder.initHidden(batch_size)\n",
    "    decoder_input = Variable(torch.LongTensor([X_word_to_id['GO']] * batch_size))\n",
    "    if use_cuda:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    # Encode\n",
    "    encoder_outputs, encoder_states = encoder(X_input, encoder_states)\n",
    "    decoder_states = encoder_states\n",
    "\n",
    "    # Decode\n",
    "    if np.random.random() <= teacher_forcing_prob:\n",
    "        # Teacher forcing: use the true label as the next decoder input\n",
    "        for i in range(y_seq_length):\n",
    "            decoder_output, decoder_states, decoder_attention = decoder(decoder_input, decoder_states, encoder_outputs)\n",
    "            loss += criterion(decoder_output, y_input[:, i])\n",
    "            decoder_input = y_input[:, i]\n",
    "    else:\n",
    "        # Otherwise, use the previous prediction\n",
    "        for i in range(y_seq_length):\n",
    "            decoder_output, decoder_states, decoder_attention = decoder(decoder_input, decoder_states, encoder_outputs)\n",
    "            loss += criterion(decoder_output, y_input[:, i])\n",
    "            \n",
    "            # Generate prediction\n",
    "            top_value, top_index = decoder_output.data.topk(1)\n",
    "            decoder_input = Variable(top_index.squeeze(1))\n",
    "            if use_cuda:\n",
    "                decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / y_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train an epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(X, y, batch_size, encoder, decoder, lr=0.01, teacher_forcing_prob=0.5):\n",
    "    total_loss = 0\n",
    "    \n",
    "    encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=lr)\n",
    "    decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    if use_cuda:\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    # loop over batches\n",
    "    epoch_size = len(X) // batch_size\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        X_batch, y_batch = get_batch(X, y, i, batch_size)\n",
    "        \n",
    "        X_batch = Variable(torch.from_numpy(X_batch).long())\n",
    "        y_batch = Variable(torch.from_numpy(y_batch).long())\n",
    "        \n",
    "        if use_cuda:\n",
    "            X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "        \n",
    "        loss = train(X_batch, y_batch, encoder, decoder, encoder_optimizer,\n",
    "                     decoder_optimizer, criterion, teacher_forcing_prob)\n",
    "        \n",
    "        total_loss += loss\n",
    "        \n",
    "    return total_loss / epoch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X_input, encoder, decoder, max_len):\n",
    "    # Initialize variables\n",
    "    batch_size, X_seq_length = X_input.size()\n",
    "    \n",
    "    encoder_states = encoder.initHidden(batch_size)\n",
    "    decoder_input = Variable(torch.LongTensor([X_word_to_id['GO']] * batch_size))\n",
    "    if use_cuda:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    # Encode\n",
    "    encoder_outputs, encoder_states = encoder(X_input, encoder_states)\n",
    "    decoder_states = encoder_states\n",
    "\n",
    "    # Decode\n",
    "    decoded_words = np.zeros((batch_size, max_len))\n",
    "    decoder_attentions = np.zeros((batch_size, max_len, max_len))\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        decoder_output, decoder_states, decoder_attention = decoder(decoder_input, decoder_states, encoder_outputs)\n",
    "        \n",
    "        # Generate prediction\n",
    "        top_value, top_index = decoder_output.data.topk(1)\n",
    "        decoded_words[:, i] = top_index.squeeze(1).cpu().numpy()\n",
    "        decoder_attentions[:, i, :] = decoder_attention.data.cpu().numpy()\n",
    "        \n",
    "        # Use the prediction as the next decoder input\n",
    "        decoder_input = Variable(top_index.squeeze(1))\n",
    "        if use_cuda:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    return decoded_words, decoder_attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "max_len = len(X_id_padded[0])\n",
    "batch_size = 100\n",
    "hidden_size = 200\n",
    "learning_rate = 0.005\n",
    "teacher_forcing_prob = 0.5\n",
    "\n",
    "encoder = EncoderRNN(X_embeddings, hidden_size)\n",
    "decoder = AttnDecoderRNN(y_embeddings, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(EncoderRNN (\n",
       "   (embedding): Embedding(13512, 200)\n",
       "   (lstm): LSTM(200, 200, num_layers=2, batch_first=True)\n",
       " ), AttnDecoderRNN (\n",
       "   (embedding): Embedding(20003, 200)\n",
       "   (new_input): Linear (400 -> 200)\n",
       "   (lstm): LSTM(200, 200, num_layers=2)\n",
       "   (out): Linear (200 -> 20003)\n",
       " ))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly pick sentences from test sets for testing\n",
    "X_ix = [i for i, e in enumerate(X_id_padded_test) if X_word_to_id['UNK'] not in e]\n",
    "y_ix = [i for i, e in enumerate(y_id_padded_test) if y_word_to_id['UNK'] not in e]\n",
    "ix = list(set(X_ix).intersection(y_ix))\n",
    "\n",
    "np.random.seed(123456)\n",
    "np.random.shuffle(ix)\n",
    "ix = ix[:3]\n",
    "\n",
    "X_test = X_id_padded_test[ix]\n",
    "y_test = y_id_padded_test[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Translate test sentences and visualize attention\n",
    "def translate_tests(X_test, y_test):\n",
    "    X_test_var = Variable(torch.from_numpy(X_test).long())\n",
    "    if use_cuda:\n",
    "        X_test_var = X_test_var.cuda()\n",
    "    translations, decoder_attentions = evaluate(X_test_var, encoder, decoder, max_len)\n",
    "    \n",
    "    input_sentences = []\n",
    "    target_sentences = []\n",
    "    output_sentences = []\n",
    "    \n",
    "    for t in range(X_test.shape[0]):\n",
    "        input_sentence = ' '.join([X_id_to_word[ix] for ix in X_test[t] if ix > 0])\n",
    "        target_sentence = ' '.join([y_id_to_word[ix] for ix in y_test[t] if ix > 0])\n",
    "        \n",
    "        # Cut off translations at the first 'ZERO' padding\n",
    "        first_zero_ix = np.where(translations[t] == 0)[0]\n",
    "        if len(first_zero_ix) > 0:\n",
    "            output_sentence = ' '.join([y_id_to_word[ix] for ix in translations[t][:first_zero_ix[0]]])\n",
    "        else:\n",
    "            output_sentence = ' '.join([y_id_to_word[ix] for ix in translations[t]])\n",
    "        \n",
    "        input_sentences.append(input_sentence)\n",
    "        target_sentences.append(target_sentence)\n",
    "        output_sentences.append(output_sentence)\n",
    "        \n",
    "    return input_sentences, target_sentences, output_sentences, decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "First sentence in English: she forgot that she had promised to call him last night\n",
      "First sentence in French: elle a oublié qu'elle avait promis de l'appeler la nuit passée\n",
      "\n",
      "Training loss: 2.1489104480784973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Runze/anaconda/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type EncoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/Runze/anaconda/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type AttnDecoderRNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translation of i have to dress up : j'ai suis de\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux de\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il a de de de\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 1\n",
      "First sentence in English: you can't do this to us\n",
      "First sentence in French: tu ne peux pas nous faire ça\n",
      "\n",
      "Training loss: 1.8543515450554477\n",
      "\n",
      "Translation of i have to dress up : j'ai me suis\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux aller\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il a fait de qu'il à\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 2\n",
      "First sentence in English: i can't imagine living like that\n",
      "First sentence in French: je ne peux pas imaginer de vivre ainsi\n",
      "\n",
      "Training loss: 1.6750223109924718\n",
      "\n",
      "Translation of i have to dress up : j'ai dois de un\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux aller\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il a toujours de de de\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 3\n",
      "First sentence in English: i hate my computer\n",
      "First sentence in French: je déteste mon ordinateur\n",
      "\n",
      "Training loss: 1.5781759486478906\n",
      "\n",
      "Translation of i have to dress up : je dois faut réparer\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux le rendre\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il a le avec avec avec\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 4\n",
      "First sentence in English: i lost three kilograms\n",
      "First sentence in French: je perdis trois kilos\n",
      "\n",
      "Training loss: 1.482546116531804\n",
      "\n",
      "Translation of i have to dress up : il me faut de\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux le\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il semble tout de le de de\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 5\n",
      "First sentence in English: you gotta get more organized\n",
      "First sentence in French: faut que vous soyez plus organisées\n",
      "\n",
      "Training loss: 1.414174645243127\n",
      "\n",
      "Translation of i have to dress up : je dois faut des\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux le\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il vit à le avec de la\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 6\n",
      "First sentence in English: i can't run as fast as he can\n",
      "First sentence in French: je ne peux pas courir aussi vite qu'il le peut\n",
      "\n",
      "Training loss: 1.377130418457497\n",
      "\n",
      "Translation of i have to dress up : je dois faut de\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux faire\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il vit à temps avec le de\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 7\n",
      "First sentence in English: you ran a red light\n",
      "First sentence in French: vous êtes passée au feu rouge\n",
      "\n",
      "Training loss: 1.3278176432341535\n",
      "\n",
      "Translation of i have to dress up : je dois cesser de\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux mourir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il parle à la de de de\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 8\n",
      "First sentence in English: i want this photograph developed as soon as possible\n",
      "First sentence in French: je veux que cette photo soit développée aussi vite que possible\n",
      "\n",
      "Training loss: 1.2875823037556835\n",
      "\n",
      "Translation of i have to dress up : je dois ouvrir une voiture\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux le\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il travaille à avec avec de à de\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 9\n",
      "First sentence in English: i had to stop\n",
      "First sentence in French: il me fallait arrêter\n",
      "\n",
      "Training loss: 1.2502602552276816\n",
      "\n",
      "Translation of i have to dress up : il me faut réparer\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux apprendre\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il passe à parler de les de de\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 10\n",
      "First sentence in English: you will never be too old to learn\n",
      "First sentence in French: on n'est jamais trop vieux pour apprendre\n",
      "\n",
      "Training loss: 1.2154322962875197\n",
      "\n",
      "Translation of i have to dress up : il me faut renouveler\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux étudier\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il met à ses autres à à à\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 11\n",
      "First sentence in English: i do love you\n",
      "First sentence in French: je t'aime\n",
      "\n",
      "Training loss: 1.1879568663038198\n",
      "\n",
      "Translation of i have to dress up : je dois prendre une\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux le\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il met à des à de à\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 12\n",
      "First sentence in English: i look forward to my birthday\n",
      "First sentence in French: je me réjouis en vue de mon anniversaire\n",
      "\n",
      "Training loss: 1.1444743665474948\n",
      "\n",
      "Translation of i have to dress up : je dois faut traire\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux devenir marier\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il prend tout les monde en monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 13\n",
      "First sentence in English: i still need to talk to you\n",
      "First sentence in French: il me faut encore te parler\n",
      "\n",
      "Training loss: 1.1246840940321707\n",
      "\n",
      "Translation of i have to dress up : je dois me mettre\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux devenir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il met ses parents avec ses monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 14\n",
      "First sentence in English: we should go home\n",
      "First sentence in French: nous devrions rentrer\n",
      "\n",
      "Training loss: 1.0988332562456984\n",
      "\n",
      "Translation of i have to dress up : je dois prendre nouveaux lecture\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux courir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il met avec avec ses ses monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 15\n",
      "First sentence in English: i ran into an old friend at tokyo station\n",
      "First sentence in French: j'ai rencontré par hasard un vieil ami à la gare de tokyo\n",
      "\n",
      "Training loss: 1.1381970004578308\n",
      "\n",
      "Translation of i have to dress up : je dois payer payer\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux dormir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il parle avec avec ses avec ses\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 16\n",
      "First sentence in English: she wiped away her tears\n",
      "First sentence in French: elle essuya ses larmes\n",
      "\n",
      "Training loss: 1.0904613852241207\n",
      "\n",
      "Translation of i have to dress up : je dois prendre une carte\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux aller\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il parle tout avec à ses monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 17\n",
      "First sentence in English: i hope you know that the last thing i want to do is hurt you\n",
      "First sentence in French: j'espère que vous savez que la dernière chose que je veuille faire est de vous blesser\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 1.079393038406871\n",
      "\n",
      "Translation of i have to dress up : je dois payer d'avance\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux courir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il met courir avec les monde monde monde monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 18\n",
      "First sentence in English: i think i can fix this\n",
      "First sentence in French: je pense que je peux arranger ça\n",
      "\n",
      "Training loss: 1.055570220635607\n",
      "\n",
      "Translation of i have to dress up : je dois acheter de verre\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux courir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il passe avec avec avec avec ses monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 19\n",
      "First sentence in English: he hurried in order to get the bus\n",
      "First sentence in French: il se hâta pour attraper le bus\n",
      "\n",
      "Training loss: 1.0282297767065713\n",
      "\n",
      "Translation of i have to dress up : je dois me mettre\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux courir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il n'arrête à les amis avec le monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 20\n",
      "First sentence in English: you will wish you had never seen it\n",
      "First sentence in French: vous UNK ne l'avoir jamais vu\n",
      "\n",
      "Training loss: 1.0201699088601501\n",
      "\n",
      "Translation of i have to dress up : je dois faut d'avance\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux courir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il vit à paris à monde à monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 21\n",
      "First sentence in English: i put some cookies on the table and the kids ate them right up\n",
      "First sentence in French: j'ai mis quelques biscuits sur la table et les enfants les ont immédiatement UNK\n",
      "\n",
      "Training loss: 1.0002822719108566\n",
      "\n",
      "Translation of i have to dress up : je dois me mettre\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux courir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il vit avec avec les espagnol et son monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 22\n",
      "First sentence in English: i don't need a girlfriend\n",
      "First sentence in French: je n'ai pas besoin de petite copine\n",
      "\n",
      "Training loss: 0.9871200687225614\n",
      "\n",
      "Translation of i have to dress up : je dois me réparer\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux me\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il passe à avec le monde de le monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 23\n",
      "First sentence in English: he has the habit of standing up when he is angry\n",
      "First sentence in French: il a l'habitude de se lever lorsqu'il est en colère\n",
      "\n",
      "Training loss: 0.9717608674159497\n",
      "\n",
      "Translation of i have to dress up : je dois me prévenir\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux courir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il parle avec avec les monde et se monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 24\n",
      "First sentence in English: she took him for all his money\n",
      "First sentence in French: elle l'a dépouillé\n",
      "\n",
      "Training loss: 0.9710188496865996\n",
      "\n",
      "Translation of i have to dress up : je dois me concentrer\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux me marier\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il vit parfois avec le monde de se paix\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 25\n",
      "First sentence in English: she may not be aware of the danger\n",
      "First sentence in French: elle n'est peut être pas consciente du danger\n",
      "\n",
      "Training loss: 0.961359272335609\n",
      "\n",
      "Translation of i have to dress up : je dois faut réparer\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux me\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il vit lentement avec avec le monde monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 26\n",
      "First sentence in English: i believe you have my umbrella\n",
      "First sentence in French: je crois que tu as mon parapluie\n",
      "\n",
      "Training loss: 0.9480764822243081\n",
      "\n",
      "Translation of i have to dress up : je dois admettre trente\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux courir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il parle courir avec les monde avec ses monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 27\n",
      "First sentence in English: i need some mental stimulation\n",
      "First sentence in French: j'ai besoin de stimulation mentale\n",
      "\n",
      "Training loss: 0.9346279990958754\n",
      "\n",
      "Translation of i have to dress up : je dois faut réparer\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux courir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il parle parfois avec les avec monde et le monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 28\n",
      "First sentence in English: he is in tokyo\n",
      "First sentence in French: il est à tokyo\n",
      "\n",
      "Training loss: 0.9305386453931886\n",
      "\n",
      "Translation of i have to dress up : je dois acheter acheter\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux parcourir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il trouve tout et tout à à monde monde monde monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 29\n",
      "First sentence in English: i don't feel like singing\n",
      "First sentence in French: je ne suis pas d'humeur à chanter\n",
      "\n",
      "Training loss: 0.9172356957703633\n",
      "\n",
      "Translation of i have to dress up : je dois acheter une\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux courir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il vit parfois et avec tout à en monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 30\n",
      "First sentence in English: i didn't mean to challenge your authority\n",
      "First sentence in French: je ne voulais pas remettre en cause votre autorité\n",
      "\n",
      "Training loss: 0.9265940554001756\n",
      "\n",
      "Translation of i have to dress up : je dois me renouveler\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux courir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il vit parfois et tous à se monde de pied\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 31\n",
      "First sentence in English: i hid under the table\n",
      "First sentence in French: je me cachai sous la table\n",
      "\n",
      "Training loss: 0.9191918145597369\n",
      "\n",
      "Translation of i have to dress up : je dois boire de\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux courir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il est tout et temps et se temps se monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 32\n",
      "First sentence in English: we don't want them to decrease our paycheck\n",
      "First sentence in French: nous ne voulons pas qu'ils réduisent notre paie\n",
      "\n",
      "Training loss: 0.9134976311187081\n",
      "\n",
      "Translation of i have to dress up : je dois me mettre\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux courir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il vit près tous tous tous tous de de\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 33\n",
      "First sentence in English: i don't know where it is\n",
      "First sentence in French: je ne sais pas où c'est\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.9041236970938896\n",
      "\n",
      "Translation of i have to dress up : je dois acheter d'acheter\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux me\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il vit moi avec avec les avec les propres\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 34\n",
      "First sentence in English: i have low blood pressure\n",
      "First sentence in French: j'ai une tension basse\n",
      "\n",
      "Training loss: 0.9007305180584946\n",
      "\n",
      "Translation of i have to dress up : je dois me mettre\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux courir\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il vit tout tous tous tous tous tous tous\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 35\n",
      "First sentence in English: i just want a vacation\n",
      "First sentence in French: je veux simplement des vacances\n",
      "\n",
      "Training loss: 0.8825015362051845\n",
      "\n",
      "Translation of i have to dress up : je dois faut une\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux me\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il s'est et et et et tous à pied\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 36\n",
      "First sentence in English: she took up his offer\n",
      "First sentence in French: elle accepta sa proposition\n",
      "\n",
      "Training loss: 0.8758520344503569\n",
      "\n",
      "Translation of i have to dress up : je dois cesser de\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux me\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il se tous tous tous tous tous tous monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 37\n",
      "First sentence in English: we have to crack down on illegal trading\n",
      "First sentence in French: il faut prendre des mesures plus fermes contre le commerce illégal\n",
      "\n",
      "Training loss: 0.8562954084026525\n",
      "\n",
      "Translation of i have to dress up : je dois me UNK\n",
      "Actual translation: je dois me faire beau\n",
      "\n",
      "Translation of i want to run : je veux me\n",
      "Actual translation: je veux courir\n",
      "\n",
      "Translation of he makes friends with everybody he meets : il met à tous tous les monde à le monde\n",
      "Actual translation: il se lie d'amitié avec tous ceux qu'il rencontre\n",
      "Epoch: 38\n",
      "First sentence in English: he was curious about how it would taste so he took a small bite\n",
      "First sentence in French: il était curieux du goût que ça aurait alors il en mordit un petit morceau\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    print('Epoch:', i)\n",
    "    \n",
    "    # Shuffle the training data every epoch to avoid local minima\n",
    "    np.random.seed(i)\n",
    "    ix = np.arange(len(X_id_padded_train))\n",
    "    np.random.shuffle(ix)\n",
    "    \n",
    "    X_id_padded_train, y_id_padded_train = X_id_padded_train[ix], y_id_padded_train[ix]\n",
    "    \n",
    "    # Print out the first sentence in X and y for sanity check\n",
    "    print('First sentence in English:', ' '.join([X_id_to_word[ix] for ix in X_id_padded_train[0] if ix > 0]))\n",
    "    print('First sentence in French:', ' '.join([y_id_to_word[ix] for ix in y_id_padded_train[0] if ix > 0]))    \n",
    "    \n",
    "    # Train an epoch    \n",
    "    train_loss = train_epoch(X_id_padded_train, y_id_padded_train, batch_size,\n",
    "                             encoder, decoder, learning_rate, teacher_forcing_prob)\n",
    "    \n",
    "    print('\\nTraining loss:', train_loss)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    torch.save(encoder, 'output/encoder_' + str(i))\n",
    "    torch.save(decoder, 'output/decoder_' + str(i))\n",
    "    \n",
    "    # Evaluate\n",
    "    # Translate test sentences\n",
    "    input_sentences, target_sentences, output_sentences, attention_plots = translate_tests(X_test, y_test)\n",
    "    \n",
    "    for j in range(len(input_sentences)):\n",
    "        print('\\nTranslation of', input_sentences[j], ':', output_sentences[j])\n",
    "        print('Actual translation:', target_sentences[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
