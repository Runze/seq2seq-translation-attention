{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import math\n",
    "from gensim.models import word2vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previously processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload data that was processed last time\n",
    "pickle_file = 'data/training_data.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    \n",
    "    X_small = save['X_small']\n",
    "    y_small = save['y_small']\n",
    "    \n",
    "    del save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we need a new initiative from the commission on this\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(word for word in X_small[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il nous faut une nouvelle initiative de la commission à ce sujet\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(word for word in y_small[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word-to-index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_to_id_mapping(data, max_vocab_size = 20000):\n",
    "    counter = collections.Counter(np.hstack(data))\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    # Pick the most common ones\n",
    "    count_pairs = count_pairs[:max_vocab_size]\n",
    "\n",
    "    # Add 'ZERO', 'GO', and 'UNK'\n",
    "    # It is important to add 'ZERO' in the beginning\n",
    "    # to make sure zero padding does not interfere with existing words\n",
    "    count_pairs.insert(0, ('GO', 0))\n",
    "    count_pairs.insert(0, ('ZERO', 0))\n",
    "    count_pairs.append(('UNK', 0))\n",
    "\n",
    "    # Create mapping for both directions\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    id_to_word = dict(zip(range(len(words)), words))\n",
    "    \n",
    "    # Map words to indexes\n",
    "    data_id = [[word_to_id[word] if word in word_to_id else word_to_id['UNK'] for word in sentence] for sentence in data]\n",
    "    \n",
    "    return word_to_id, id_to_word, data_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_to_id, X_id_to_word, X_id = create_word_to_id_mapping(X_small)\n",
    "y_word_to_id, y_id_to_word, y_id = create_word_to_id_mapping(y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000 20003 20003\n"
     ]
    }
   ],
   "source": [
    "print(len(X_id), len(y_id), len(X_word_to_id), len(y_word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vocab_size, y_vocab_size = len(X_word_to_id), len(y_word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we need a new initiative from the commission on this\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([X_id_to_word[i] for i in X_id[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il nous faut une nouvelle initiative de la commission à ce sujet\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([y_id_to_word[i] for i in y_id[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad zeros to make sentences equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50  # As defined last time\n",
    "X_id_padded = pad_sequences(X_id, maxlen=max_len, padding='post')\n",
    "y_id_padded = pad_sequences(y_id, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_id_padded_train, X_id_padded_test, y_id_padded_train, y_id_padded_test = model_selection.train_test_split(\n",
    "    X_id_padded, y_id_padded, test_size=0.1, random_state=123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(90000, 50), (10000, 50), (90000, 50), (10000, 50)]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.shape for e in (X_id_padded_train, X_id_padded_test, y_id_padded_train, y_id_padded_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i would like to draw attention to one of the commission's most important commitments to reduce poverty in europe and increase social inclusion ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([X_id_to_word[i] for i in X_id_padded_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je souhaite attirer votre attention sur un des engagements les plus importants de la commission à savoir la réduction de la pauvreté en europe et le renforcement de l'inclusion sociale ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO ZERO\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([y_id_to_word[i] for i in y_id_padded_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverage pre-trained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English word vectors downloaded from https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code stolen from https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "embeddings_index_en = {}\n",
    "f = open('data/glove.6B/glove.6B.200d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index_en[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French word vectors downloaded from http://fauconnier.github.io/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index_fr = word2vec.KeyedVectors.load_word2vec_format(\n",
    "    'data/frWac_non_lem_no_postag_no_phrase_200_skip_cut100.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map words to pre-trained embeddings\n",
    "def map_word_to_pretrained_embedding(embeddings_index, embeddings_dim, word_to_id):\n",
    "    vocab_size = len(word_to_id)\n",
    "    embedding_matrix = np.zeros((vocab_size, embeddings_dim))\n",
    "    \n",
    "    # Keep a running count of matched words\n",
    "    found = 0\n",
    "    \n",
    "    for word, i in word_to_id.items():\n",
    "        if word in embeddings_index:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            found += 1\n",
    "        else:\n",
    "            # Words not found in embedding index will be randomly initialized\n",
    "            embedding_matrix[i] = np.random.normal(size=(embedding_size, ))\n",
    "\n",
    "    return embedding_matrix, found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20003, 200), 18053)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embeddings, X_found = map_word_to_pretrained_embedding(embeddings_index_en, 200, X_word_to_id)\n",
    "X_embeddings.shape, X_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20003, 200), 17287)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_embeddings, y_found = map_word_to_pretrained_embedding(embeddings_index_fr, 200, y_word_to_id)\n",
    "y_embeddings.shape, y_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'data/training_data.pickle'\n",
    "\n",
    "f = open(pickle_file, 'wb')\n",
    "save = {\n",
    "    'X_small': X_small,\n",
    "    'y_small': y_small,\n",
    "    'X_word_to_id': X_word_to_id,\n",
    "    'X_id_to_word': X_id_to_word,\n",
    "    'y_word_to_id': y_word_to_id,\n",
    "    'y_id_to_word': y_id_to_word,\n",
    "    'X_id_padded': X_id_padded,\n",
    "    'y_id_padded': y_id_padded,\n",
    "    'X_embeddings': X_embeddings,\n",
    "    'y_embeddings': y_embeddings\n",
    "}\n",
    "\n",
    "pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reload data that was processed last time\n",
    "pickle_file = 'data/training_data.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    \n",
    "    X_small = save['X_small']\n",
    "    y_small = save['y_small']\n",
    "    X_word_to_id = save['X_word_to_id']\n",
    "    X_id_to_word = save['X_id_to_word']\n",
    "    y_word_to_id = save['y_word_to_id']\n",
    "    y_id_to_word = save['y_id_to_word']\n",
    "    X_id_padded = save['X_id_padded']\n",
    "    y_id_padded = save['y_id_padded']\n",
    "    X_embeddings = save['X_embeddings']\n",
    "    y_embeddings = save['y_embeddings']\n",
    "    \n",
    "    del save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20003, 200)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a embedding layer initialized with pre-trained embedding matrix\n",
    "def create_embedding(init_embeddings, trainable=True):\n",
    "    vocab_size, embedding_size = init_embeddings.shape\n",
    "    embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "    \n",
    "    # Convert pre-trained embeddings to a tensor\n",
    "    # init_embeddings = torch.FloatTensor(init_embeddings).cuda()\n",
    "    init_embeddings = torch.FloatTensor(init_embeddings)\n",
    "    embedding.load_state_dict({'weight': init_embeddings})\n",
    "    \n",
    "    if not trainable:\n",
    "        for param in embeddings.parameters(): \n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return embedding, vocab_size, embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(20003, 200), 20003, 200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dimensions\n",
    "create_embedding(X_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create encoder RNN using LSTM\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, init_embeddings, hidden_size, n_layers=2):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding, vocab_size, embedding_size = create_embedding(init_embeddings)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, input, states):\n",
    "        output, states = self.lstm(self.embedding(input), states)\n",
    "        return output, states\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        init_hidden_state = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "        init_cell_state = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "        return (init_hidden_state, init_cell_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly initialized weight matrices\n",
    "def arr(*size):\n",
    "    return torch.randn(size) / math.sqrt(size[0])\n",
    "\n",
    "def param(*size):\n",
    "    # return nn.Parameter(arr(*size)).cuda()\n",
    "    return nn.Parameter(arr(*size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy style dot operation to multiply a 3D matrix with a 2D one\n",
    "# Based on https://discuss.pytorch.org/t/how-can-i-compute-3d-tensor-2d-tensor-multiplication/639/9\n",
    "def dot(X, Y):\n",
    "    return torch.bmm(X, Y.unsqueeze(0).expand(X.size(0), *Y.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$u^t_i = v^T tanh(W_1′ h_i + W_2′ d_t)$$\n",
    "$$a^t_i = softmax(u^t_i)$$\n",
    "$$d_t' = \\sum_i^{T_A} a^t_i h_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, init_embeddings, hidden_size, n_layers=2):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.embedding, vocab_size, embedding_size = create_embedding(init_embeddings)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Define weights and intercepts used in paper 1412.7449\n",
    "        # to construct the allignment matrix: u^t_i = v^T tanh(W_1′ h_i + W_2′ d_t)\n",
    "        self.W1 = param(hidden_size, hidden_size)\n",
    "        self.W2 = param(hidden_size, hidden_size)\n",
    "        self.b = param(hidden_size)\n",
    "        self.v = param(hidden_size)\n",
    "        \n",
    "        # Linear layer to reshape hidden state, concatenated with either the previous true label or prediction,\n",
    "        # back to the shape of hidden state\n",
    "        # As the new input to LSTM\n",
    "        self.new_input = nn.Linear(hidden_size + embedding_size, hidden_size)\n",
    "        \n",
    "        # LSTM layers using the new concatenated hidden state as the input\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "        # Linear layer to reshape data to the shape of output vocabulary\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input, states, encoder_outputs):\n",
    "        # u^t_i = v^T tanh(W_1′ h_i + W_2′ d_t)\n",
    "        W1h = dot(encoder_outputs, self.W1)        # shape = (batch_size, seq_length, hidden_size)\n",
    "        hidden_state = states[0]                   # shape = (n_layers, batch_size, hidden_size)\n",
    "        W2d = hidden_state[-1].mm(self.W2)         # shape = (batch_size, hidden_size)\n",
    "        W1h_W2d = W1h + W2d.unsqueeze(1) + self.b  # shape = (batch_size, seq_length, hidden_size)\n",
    "        tahn_W1h_W2d = F.tanh(W1h_W2d)             # shape = (batch_size, seq_length, hidden_size)\n",
    "        u = (tahn_W1h_W2d * self.v).sum(2)         # shape = (batch_size, seq_length)\n",
    "        \n",
    "        # a^t_i = softmax(u^t_i)\n",
    "        a = F.softmax(u)                           # shape = (batch_size, seq_length)\n",
    "        \n",
    "        # d_t' = \\sum_i^{T_A} a^t_i h_i\n",
    "        weighted_encoder_outputs = (a.unsqueeze(2) * encoder_outputs).sum(1)  # shape = (batch_size, hidden_size)\n",
    "        \n",
    "        # Concatenate with decoder input,\n",
    "        # which is either the previous true label or prediction\n",
    "        concat_input = torch.cat((weighted_encoder_outputs, self.embedding(input)), 1)  # shape = (batch_size, hidden_size + embedding_size)\n",
    "        \n",
    "        # Reshape the concatenated input back to the shape of hidden state\n",
    "        reshaped_input = self.new_input(concat_input)   # shape = (batch_size, hidden_size)\n",
    "        \n",
    "        # Feed the new input into the LSTM layer\n",
    "        output, states = self.lstm(reshaped_input.unsqueeze(0), states)\n",
    "        output = output.squeeze(0)                 # shape = (batch_size, hidden_size)\n",
    "        \n",
    "        # Finally, feed to the output layer\n",
    "        output = self.out(output)                  # shape = (batch_size, vocab_size)\n",
    "        \n",
    "        return output, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X_input, y_input, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion, teacher_forcing_prob):\n",
    "    # Initialize variables\n",
    "    batch_size, X_seq_length = X_input.size()\n",
    "    y_seq_length = y_input.size()[1]\n",
    "\n",
    "    # encoder_states = encoder.initHidden(batch_size).cuda()\n",
    "    encoder_states = encoder.initHidden(batch_size)\n",
    "    decoder_input = Variable(torch.LongTensor([X_word_to_id['GO']] * batch_size))\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    # Encode\n",
    "    encoder_outputs, encoder_states = encoder(X_input, encoder_states)\n",
    "    decoder_states = encoder_states\n",
    "\n",
    "    # Decode\n",
    "    for i in range(y_seq_length):\n",
    "        decoder_output, decoder_states = decoder(decoder_input, decoder_states, encoder_outputs)\n",
    "        loss += criterion(decoder_output, y_input[:, i])\n",
    "        \n",
    "        if np.random.random() < teacher_forcing_prob:\n",
    "            # Teacher forcing: use the true label as the next decoder input\n",
    "            decoder_input = y_input[:, i]\n",
    "        else:\n",
    "            # Otherwise, use the previous prediction\n",
    "            top_value, top_index = decoder_output.data.topk(1)\n",
    "            decoder_input = Variable(top_index.squeeze(1))\n",
    "        \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / y_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.829857788085938"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(X_input=Variable(torch.from_numpy(X_id_padded_b).long()),\n",
    "     y_input=Variable(torch.from_numpy(X_id_padded_b).long()),\n",
    "     encoder=EncoderRNN(X_embeddings, hidden_size),\n",
    "     decoder=AttnDecoderRNN(y_embeddings, hidden_size),\n",
    "     encoder_optimizer=encoder_optimizer,\n",
    "     decoder_optimizer=decoder_optimizer,\n",
    "     criterion=nn.CrossEntropyLoss(),\n",
    "     teacher_forcing_prob=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size=16):\n",
    "    idxs = np.random.permutation(len(x))[:batch_size]\n",
    "    return x[idxs], y[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "X_id_padded_b, y_id_padded_b = get_batch(X_id_padded, y_id_padded, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 10\n",
    "encoder = EncoderRNN(X_embeddings, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = Variable(torch.from_numpy(X_id_padded_b).long())\n",
    "states = encoder.initHidden(batch_size)\n",
    "encoder_outputs, states = encoder(X_input, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = AttnDecoderRNN(y_embeddings, hidden_size)\n",
    "\n",
    "y_input = Variable(torch.from_numpy(y_id_padded_b).long())[:, 0]\n",
    "decoder_output, states = decoder(y_input, states, encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.5822e-02  2.6860e-01  1.7302e-01  ...  -1.9393e-01 -1.9231e-01 -2.0569e-01\n",
       " 2.5806e-02  2.6883e-01  1.7236e-01  ...  -1.9423e-01 -1.9162e-01 -2.0565e-01\n",
       " 2.5578e-02  2.6874e-01  1.7252e-01  ...  -1.9496e-01 -1.9219e-01 -2.0582e-01\n",
       "[torch.FloatTensor of size 3x20003]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_value, top_index = decoder_output.data.topk(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  0.4365\n",
       "  0.4360\n",
       "  0.4359\n",
       " [torch.FloatTensor of size 3x1], \n",
       "  17910\n",
       "  17910\n",
       "  17910\n",
       " [torch.LongTensor of size 3x1])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_value, top_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 200])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(Variable(top_index.squeeze(1))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, vocab_size, embedding_size = create_embedding(y_embeddings)\n",
    "\n",
    "# Define weights and intercepts used in paper 1412.7449\n",
    "# to construct the allignment matrix: u^t_i = v^T tanh(W_1′ h_i + W_2′ d_t)\n",
    "W1 = param(hidden_size, hidden_size)\n",
    "W2 = param(hidden_size, hidden_size)\n",
    "b = param(hidden_size)\n",
    "v = param(hidden_size)\n",
    "\n",
    "# Weights to reshape hidden state, concatenated with either the previous true label or prediction,\n",
    "# back to the shape of hidden state\n",
    "W3 = param(hidden_size + embedding_size, hidden_size)\n",
    "b3 = param(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 50, 10]), torch.Size([10, 10]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.size(), W1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50, 10])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1h = dot(encoder_outputs, W1)\n",
    "W1h.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10]), torch.Size([10, 10]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state = states[0]\n",
    "hidden_state[-1].size(), W2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2d = hidden_state[-1].mm(W2)\n",
    "W2d.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50, 10])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1h_W2d = W1h + W2d.unsqueeze(1) + b\n",
    "W1h_W2d.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50, 10])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tahn_W1h_W2d = F.tanh(W1h_W2d)\n",
    "tahn_W1h_W2d.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = (tahn_W1h_W2d * v).sum(2)\n",
    "u.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 50])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = F.softmax(u)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_encoder_outputs = (a.unsqueeze(2) * encoder_outputs).sum(1)\n",
    "weighted_encoder_outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = input[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 200])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(input).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 210])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_input = torch.cat((weighted_encoder_outputs, embedding(input)), 1)\n",
    "concat_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_input = nn.Linear(hidden_size + embedding_size, hidden_size)\n",
    "reshaped_input = new_input(concat_input)\n",
    "reshaped_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "output, states = lstm(reshaped_input.unsqueeze(0), states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 10])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0945  0.1036  0.0964  0.1101  0.1028  0.0979  0.1014  0.0943  0.0928  0.1061\n",
       " 0.0945  0.1036  0.0964  0.1101  0.1028  0.0979  0.1014  0.0943  0.0928  0.1061\n",
       " 0.0945  0.1036  0.0964  0.1101  0.1028  0.0979  0.1014  0.0943  0.0928  0.1061\n",
       "[torch.FloatTensor of size 3x10]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(output.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
